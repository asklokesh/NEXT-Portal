# Comprehensive Disaster Recovery System
# Enterprise-grade DR with <15 minute RTO and 99.99% uptime SLA

---
# DR Configuration Map
apiVersion: v1
kind: ConfigMap
metadata:
  name: dr-configuration
  namespace: disaster-recovery
data:
  rto-target: "15m"  # Recovery Time Objective
  rpo-target: "5m"   # Recovery Point Objective
  backup-frequency: "5m"
  cross-region-replication: "true"
  automated-failover: "true"
  failback-validation: "true"
  dr-testing-schedule: "0 2 * * 0"  # Weekly DR testing
  primary-region: "us-east-1"
  secondary-region: "us-west-2"
  tertiary-region: "eu-central-1"

---
# Primary Backup Job (Continuous)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: continuous-backup
  namespace: disaster-recovery
  annotations:
    cronjob.kubernetes.io/description: "Continuous backup for <5min RPO"
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  concurrencyPolicy: Replace
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      activeDeadlineSeconds: 300  # 5 minutes max
      template:
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account
          containers:
          - name: backup-executor
            image: backup-orchestrator:latest
            command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "Starting continuous backup at $(date)"
              
              # Database backup with point-in-time recovery
              echo "Creating database backup..."
              pg_dump $DATABASE_URL -f /backup/db-$(date +%Y%m%d-%H%M%S).sql
              
              # Application state backup
              echo "Backing up application state..."
              kubectl get all -n saas-idp-production -o yaml > /backup/k8s-state-$(date +%Y%m%d-%H%M%S).yaml
              
              # Configuration backup
              echo "Backing up configurations..."
              kubectl get configmaps,secrets -n saas-idp-production -o yaml > /backup/config-$(date +%Y%m%d-%H%M%S).yaml
              
              # Upload to S3 with cross-region replication
              echo "Uploading to S3..."
              aws s3 sync /backup/ s3://saas-idp-backup-primary/$(date +%Y%m%d)/ --delete
              
              # Verify backup integrity
              echo "Verifying backup integrity..."
              BACKUP_SIZE=$(aws s3 ls s3://saas-idp-backup-primary/$(date +%Y%m%d)/ --recursive --summarize | grep "Total Size" | awk '{print $3}')
              if [ "$BACKUP_SIZE" -lt 1000000 ]; then  # Less than 1MB indicates failure
                echo "Backup verification failed - size too small: $BACKUP_SIZE"
                exit 1
              fi
              
              # Clean old backups (keep last 48 hours)
              find /backup -name "*.sql" -mtime +2 -delete
              find /backup -name "*.yaml" -mtime +2 -delete
              
              # Update backup status
              kubectl patch configmap backup-status -n disaster-recovery --type merge -p '{"data":{"last-backup":"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'","status":"success","size":"'$BACKUP_SIZE'"}}'
              
              echo "Continuous backup completed successfully"
            env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: url
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
            - name: AWS_DEFAULT_REGION
              value: "us-east-1"
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
            resources:
              requests:
                memory: "1Gi"
                cpu: "500m"
              limits:
                memory: "2Gi"
                cpu: "1000m"
          volumes:
          - name: backup-storage
            emptyDir:
              sizeLimit: 10Gi

---
# DR Orchestrator Service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dr-orchestrator
  namespace: disaster-recovery
  labels:
    app: dr-orchestrator
spec:
  replicas: 2
  selector:
    matchLabels:
      app: dr-orchestrator
  template:
    metadata:
      labels:
        app: dr-orchestrator
    spec:
      serviceAccountName: dr-orchestrator-sa
      containers:
      - name: orchestrator
        image: dr-orchestrator:latest
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: RTO_TARGET
          value: "15m"
        - name: RPO_TARGET
          value: "5m"
        - name: PRIMARY_REGION
          value: "us-east-1"
        - name: SECONDARY_REGION
          value: "us-west-2"
        - name: MONITORING_INTERVAL
          value: "30s"
        - name: FAILOVER_THRESHOLD
          value: "3"  # Failed health checks before failover
        - name: PROMETHEUS_URL
          value: "http://prometheus.monitoring.svc.cluster.local:9090"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 5
        volumeMounts:
        - name: dr-scripts
          mountPath: /scripts
        - name: kubeconfig
          mountPath: /root/.kube
      volumes:
      - name: dr-scripts
        configMap:
          name: dr-scripts
          defaultMode: 0755
      - name: kubeconfig
        secret:
          secretName: kubeconfig-multi-cluster

---
# DR Scripts ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: dr-scripts
  namespace: disaster-recovery
data:
  failover.sh: |
    #!/bin/bash
    set -e
    
    echo "Initiating failover to secondary region..."
    START_TIME=$(date +%s)
    
    # 1. Verify primary region failure (2 minutes max)
    echo "Step 1: Verifying primary region failure..."
    PRIMARY_HEALTH=$(curl -f -m 10 https://app.saas-idp.company.com/health || echo "FAILED")
    if [ "$PRIMARY_HEALTH" != "FAILED" ]; then
      echo "Primary region is healthy, aborting failover"
      exit 1
    fi
    
    # 2. Activate secondary region infrastructure (3 minutes max)
    echo "Step 2: Activating secondary region infrastructure..."
    kubectl config use-context secondary-cluster
    
    # Scale up secondary region deployment
    kubectl patch hpa saas-idp-hpa -n saas-idp-production -p '{"spec":{"minReplicas":6,"maxReplicas":50}}'
    kubectl scale deployment saas-idp -n saas-idp-production --replicas=6
    
    # Wait for pods to be ready
    kubectl wait --for=condition=ready pod -l app=saas-idp -n saas-idp-production --timeout=180s
    
    # 3. Update DNS to point to secondary region (1 minute max)
    echo "Step 3: Updating DNS records..."
    aws route53 change-resource-record-sets --hosted-zone-id Z123456789 --change-batch '{
      "Changes": [{
        "Action": "UPSERT",
        "ResourceRecordSet": {
          "Name": "app.saas-idp.company.com",
          "Type": "CNAME",
          "TTL": 60,
          "ResourceRecords": [{"Value": "secondary-lb.us-west-2.elb.amazonaws.com"}]
        }
      }]
    }'
    
    # 4. Restore database from latest backup (5 minutes max)
    echo "Step 4: Restoring database..."
    LATEST_BACKUP=$(aws s3 ls s3://saas-idp-backup-primary/$(date +%Y%m%d)/ | sort | tail -n 1 | awk '{print $4}')
    aws s3 cp s3://saas-idp-backup-primary/$(date +%Y%m%d)/$LATEST_BACKUP /tmp/restore.sql
    
    # Apply backup to secondary database
    psql $SECONDARY_DATABASE_URL -f /tmp/restore.sql
    
    # 5. Verify secondary region health (2 minutes max)
    echo "Step 5: Verifying secondary region health..."
    for i in {1..24}; do  # 2 minutes with 5-second intervals
      SECONDARY_HEALTH=$(curl -f -m 10 https://app.saas-idp.company.com/health || echo "FAILED")
      if [ "$SECONDARY_HEALTH" != "FAILED" ]; then
        echo "Secondary region is healthy!"
        break
      fi
      if [ $i -eq 24 ]; then
        echo "Secondary region health check failed"
        exit 1
      fi
      sleep 5
    done
    
    # 6. Send notifications
    echo "Step 6: Sending notifications..."
    END_TIME=$(date +%s)
    FAILOVER_TIME=$((END_TIME - START_TIME))
    
    curl -X POST "$SLACK_WEBHOOK_URL" -H 'Content-type: application/json' --data '{
      "text": "ðŸš¨ FAILOVER COMPLETED",
      "attachments": [{
        "color": "warning",
        "fields": [{
          "title": "Failover Time",
          "value": "'$FAILOVER_TIME' seconds",
          "short": true
        }, {
          "title": "Target Region",
          "value": "us-west-2",
          "short": true
        }, {
          "title": "Status",
          "value": "Active",
          "short": true
        }]
      }]
    }'
    
    echo "Failover completed in $FAILOVER_TIME seconds"

  failback.sh: |
    #!/bin/bash
    set -e
    
    echo "Initiating failback to primary region..."
    START_TIME=$(date +%s)
    
    # 1. Verify primary region recovery
    echo "Step 1: Verifying primary region recovery..."
    kubectl config use-context primary-cluster
    
    # Check primary region infrastructure
    kubectl get nodes --no-headers | wc -l
    PRIMARY_NODES=$(kubectl get nodes --no-headers | grep Ready | wc -l)
    if [ "$PRIMARY_NODES" -lt 3 ]; then
      echo "Insufficient healthy nodes in primary region: $PRIMARY_NODES"
      exit 1
    fi
    
    # 2. Sync data from secondary to primary
    echo "Step 2: Syncing data from secondary to primary..."
    kubectl config use-context secondary-cluster
    
    # Create final backup from secondary
    kubectl exec -n saas-idp-production deployment/saas-idp -- pg_dump $DATABASE_URL > /tmp/failback-sync.sql
    
    # Apply to primary
    kubectl config use-context primary-cluster
    psql $PRIMARY_DATABASE_URL -f /tmp/failback-sync.sql
    
    # 3. Scale up primary region
    echo "Step 3: Scaling up primary region..."
    kubectl patch hpa saas-idp-hpa -n saas-idp-production -p '{"spec":{"minReplicas":6,"maxReplicas":50}}'
    kubectl scale deployment saas-idp -n saas-idp-production --replicas=6
    kubectl wait --for=condition=ready pod -l app=saas-idp -n saas-idp-production --timeout=300s
    
    # 4. Update DNS back to primary
    echo "Step 4: Updating DNS to primary region..."
    aws route53 change-resource-record-sets --hosted-zone-id Z123456789 --change-batch '{
      "Changes": [{
        "Action": "UPSERT",
        "ResourceRecordSet": {
          "Name": "app.saas-idp.company.com",
          "Type": "CNAME",
          "TTL": 60,
          "ResourceRecords": [{"Value": "primary-lb.us-east-1.elb.amazonaws.com"}]
        }
      }]
    }'
    
    # 5. Verify primary region health
    echo "Step 5: Verifying primary region health..."
    sleep 60  # Wait for DNS propagation
    
    for i in {1..24}; do
      PRIMARY_HEALTH=$(curl -f -m 10 https://app.saas-idp.company.com/health || echo "FAILED")
      if [ "$PRIMARY_HEALTH" != "FAILED" ]; then
        echo "Primary region is healthy!"
        break
      fi
      if [ $i -eq 24 ]; then
        echo "Primary region health check failed"
        exit 1
      fi
      sleep 5
    done
    
    # 6. Scale down secondary region
    echo "Step 6: Scaling down secondary region..."
    kubectl config use-context secondary-cluster
    kubectl scale deployment saas-idp -n saas-idp-production --replicas=1
    kubectl patch hpa saas-idp-hpa -n saas-idp-production -p '{"spec":{"minReplicas":1,"maxReplicas":3}}'
    
    END_TIME=$(date +%s)
    FAILBACK_TIME=$((END_TIME - START_TIME))
    
    echo "Failback completed in $FAILBACK_TIME seconds"

  dr-test.sh: |
    #!/bin/bash
    set -e
    
    echo "Starting DR test at $(date)"
    TEST_START=$(date +%s)
    
    # 1. Create test backup
    echo "Creating test backup..."
    kubectl exec -n saas-idp-production deployment/saas-idp -- pg_dump $DATABASE_URL > /tmp/dr-test-backup.sql
    
    # 2. Test secondary region deployment
    echo "Testing secondary region deployment..."
    kubectl config use-context secondary-cluster
    
    # Deploy test instance
    kubectl create namespace saas-idp-dr-test --dry-run=client -o yaml | kubectl apply -f -
    kubectl apply -f /manifests/dr-test-deployment.yaml -n saas-idp-dr-test
    
    # Wait for test deployment
    kubectl wait --for=condition=ready pod -l app=saas-idp-dr-test -n saas-idp-dr-test --timeout=300s
    
    # 3. Test data restoration
    echo "Testing data restoration..."
    kubectl exec -n saas-idp-dr-test deployment/saas-idp-dr-test -- psql $TEST_DATABASE_URL -f /tmp/dr-test-backup.sql
    
    # 4. Verify application functionality
    echo "Verifying application functionality..."
    TEST_POD=$(kubectl get pod -n saas-idp-dr-test -l app=saas-idp-dr-test -o jsonpath='{.items[0].metadata.name}')
    kubectl port-forward -n saas-idp-dr-test pod/$TEST_POD 8080:3000 &
    PORT_FORWARD_PID=$!
    
    sleep 10  # Wait for port-forward
    
    # Test health endpoint
    if curl -f http://localhost:8080/health; then
      echo "Health check passed"
    else
      echo "Health check failed"
      kill $PORT_FORWARD_PID
      exit 1
    fi
    
    # Test API endpoint
    if curl -f http://localhost:8080/api/health; then
      echo "API check passed"
    else
      echo "API check failed"
      kill $PORT_FORWARD_PID
      exit 1
    fi
    
    kill $PORT_FORWARD_PID
    
    # 5. Cleanup test resources
    echo "Cleaning up test resources..."
    kubectl delete namespace saas-idp-dr-test --wait=true
    
    TEST_END=$(date +%s)
    TEST_DURATION=$((TEST_END - TEST_START))
    
    # 6. Generate test report
    echo "Generating DR test report..."
    cat > /tmp/dr-test-report.json << EOF
    {
      "test_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
      "test_duration_seconds": $TEST_DURATION,
      "rto_target": "900",
      "rpo_target": "300",
      "test_results": {
        "backup_creation": "PASSED",
        "secondary_deployment": "PASSED",
        "data_restoration": "PASSED",
        "health_check": "PASSED",
        "api_functionality": "PASSED"
      },
      "estimated_rto": "$TEST_DURATION",
      "status": "PASSED"
    }
    EOF
    
    # Upload test report
    aws s3 cp /tmp/dr-test-report.json s3://saas-idp-dr-reports/$(date +%Y%m%d)/
    
    # Send notification
    curl -X POST "$SLACK_WEBHOOK_URL" -H 'Content-type: application/json' --data '{
      "text": "âœ… DR Test Completed Successfully",
      "attachments": [{
        "color": "good",
        "fields": [{
          "title": "Test Duration",
          "value": "'$TEST_DURATION' seconds",
          "short": true
        }, {
          "title": "Estimated RTO",
          "value": "'$TEST_DURATION' seconds",
          "short": true
        }, {
          "title": "Status",
          "value": "PASSED",
          "short": true
        }]
      }]
    }'
    
    echo "DR test completed successfully in $TEST_DURATION seconds"

---
# DR Test CronJob (Weekly)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-test-weekly
  namespace: disaster-recovery
spec:
  schedule: "0 2 * * 0"  # Every Sunday at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          serviceAccountName: dr-orchestrator-sa
          containers:
          - name: dr-test
            image: dr-orchestrator:latest
            command: ["/scripts/dr-test.sh"]
            env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: url
            - name: TEST_DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: test-url
            - name: SLACK_WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: notification-secrets
                  key: slack-webhook-url
            volumeMounts:
            - name: dr-scripts
              mountPath: /scripts
            - name: kubeconfig
              mountPath: /root/.kube
            - name: manifests
              mountPath: /manifests
          volumes:
          - name: dr-scripts
            configMap:
              name: dr-scripts
              defaultMode: 0755
          - name: kubeconfig
            secret:
              secretName: kubeconfig-multi-cluster
          - name: manifests
            configMap:
              name: dr-manifests

---
# Health Monitor for Failover Detection
apiVersion: apps/v1
kind: Deployment
metadata:
  name: health-monitor
  namespace: disaster-recovery
  labels:
    app: health-monitor
spec:
  replicas: 3  # Multiple replicas for reliability
  selector:
    matchLabels:
      app: health-monitor
  template:
    metadata:
      labels:
        app: health-monitor
    spec:
      containers:
      - name: monitor
        image: health-monitor:latest
        ports:
        - containerPort: 8080
        env:
        - name: PRIMARY_ENDPOINT
          value: "https://app.saas-idp.company.com/health"
        - name: CHECK_INTERVAL
          value: "10s"
        - name: FAILURE_THRESHOLD
          value: "3"
        - name: DR_WEBHOOK_URL
          value: "http://dr-orchestrator.disaster-recovery.svc.cluster.local:8080/trigger-failover"
        - name: SLACK_WEBHOOK_URL
          valueFrom:
            secretKeyRef:
              name: notification-secrets
              key: slack-webhook-url
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10

---
# Service Account for DR Operations
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dr-orchestrator-sa
  namespace: disaster-recovery
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/dr-orchestrator-role

---
# ClusterRole for DR Operations
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: dr-orchestrator-role
rules:
- apiGroups: [""]
  resources: ["*"]
  verbs: ["*"]
- apiGroups: ["apps"]
  resources: ["*"]
  verbs: ["*"]
- apiGroups: ["autoscaling"]
  resources: ["*"]
  verbs: ["*"]
- apiGroups: ["batch"]
  resources: ["*"]
  verbs: ["*"]
- apiGroups: ["networking.k8s.io"]
  resources: ["*"]
  verbs: ["*"]

---
# ClusterRoleBinding for DR Operations
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dr-orchestrator-binding
subjects:
- kind: ServiceAccount
  name: dr-orchestrator-sa
  namespace: disaster-recovery
roleRef:
  kind: ClusterRole
  name: dr-orchestrator-role
  apiGroup: rbac.authorization.k8s.io

---
# DR Metrics for Monitoring
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: dr-orchestrator
  namespace: disaster-recovery
spec:
  selector:
    matchLabels:
      app: dr-orchestrator
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# DR Alerting Rules
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: dr-alerting-rules
  namespace: disaster-recovery
spec:
  groups:
  - name: disaster-recovery.rules
    rules:
    - alert: DRBackupFailed
      expr: dr_backup_status{status="failed"} == 1
      for: 1m
      labels:
        severity: critical
        team: platform
      annotations:
        summary: "DR backup failed"
        description: "Disaster recovery backup has failed"
        runbook_url: "https://runbooks.company.com/dr/backup-failed"

    - alert: DRRTOExceeded
      expr: dr_test_rto_seconds > 900  # 15 minutes
      for: 0m
      labels:
        severity: critical
        team: platform
      annotations:
        summary: "DR RTO target exceeded"
        description: "DR test showed RTO of {{ $value }}s exceeds target of 900s"
        runbook_url: "https://runbooks.company.com/dr/rto-exceeded"

    - alert: DRTestFailed
      expr: dr_test_status{status="failed"} == 1
      for: 0m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "DR test failed"
        description: "Weekly DR test has failed"
        runbook_url: "https://runbooks.company.com/dr/test-failed"

    - alert: HealthMonitorDown
      expr: up{job="health-monitor"} == 0
      for: 2m
      labels:
        severity: critical
        team: platform
      annotations:
        summary: "Health monitor is down"
        description: "DR health monitor is not responding"
        runbook_url: "https://runbooks.company.com/dr/monitor-down"