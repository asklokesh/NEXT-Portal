# Comprehensive SLA Monitoring Stack # 99.99% uptime SLA monitoring with advanced alerting and incident management --- # SLA Configuration apiVersion: v1 kind: ConfigMap metadata: name: sla-config namespace: monitoring data: uptime-target: "99.99" # 99.99% uptime SLA error-budget-minutes: "52.56" # 52.56 minutes per year alert-threshold-critical: "99.95" # Alert when below 99.95% alert-threshold-warning: "99.97" # Warning when below 99.97% measurement-window: "30d" # 30-day rolling window incident-severity-mapping: | critical: < 99.95% high: < 99.97% medium: < 99.98% low: < 99.99% availability-targets: | api-endpoints: 99.99% web-interface: 99.99% authentication: 99.995% plugin-system: 99.98% websocket: 99.98% database: 99.995% --- # Custom SLI/SLO Metrics apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: sli-slo-metrics namespace: monitoring labels: prometheus: kube-prometheus role: alert-rules spec: groups: - name: sli-metrics.rules interval: 30s rules: # Availability SLI - record: sli:availability:ratio_rate5m expr: | sum(rate(probe_success{instance=~"https://app.saas-idp.company.com.*"}[5m])) / sum(rate(probe_success[5m])) - record: sli:availability:ratio_rate30m expr: | sum(rate(probe_success{instance=~"https://app.saas-idp.company.com.*"}[30m])) / sum(rate(probe_success[30m])) - record: sli:availability:ratio_rate1h expr: | sum(rate(probe_success{instance=~"https://app.saas-idp.company.com.*"}[1h])) / sum(rate(probe_success[1h])) - record: sli:availability:ratio_rate1d expr: | sum(rate(probe_success{instance=~"https://app.saas-idp.company.com.*"}[1d])) / sum(rate(probe_success[1d])) # Latency SLI (P95 < 200ms, P99 < 500ms) - record: sli:latency:p95_5m expr: | histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="saas-idp"}[5m])) by (le) ) - record: sli:latency:p99_5m expr: | histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{job="saas-idp"}[5m])) by (le) ) # Error Rate SLI (< 0.1% error rate) - record: sli:error_rate:ratio_rate5m expr: | sum(rate(http_requests_total{job="saas-idp",status=~"5.."}[5m])) / sum(rate(http_requests_total{job="saas-idp"}[5m])) # Throughput SLI - record: sli:throughput:requests_per_second expr: | sum(rate(http_requests_total{job="saas-idp"}[5m])) - name: slo-burn-rate.rules interval: 30s rules: # Error budget burn rate calculations - record: slo:error_budget_burn_rate:5m expr: | ( 1 - sli:availability:ratio_rate5m ) / (1 - 0.9999) * 60 * 5 # 5-minute burn rate - record: slo:error_budget_burn_rate:1h expr: | ( 1 - sli:availability:ratio_rate1h ) / (1 - 0.9999) * 60 # 1-hour burn rate - record: slo:error_budget_burn_rate:1d expr: | ( 1 - sli:availability:ratio_rate1d ) / (1 - 0.9999) # 1-day burn rate # Error budget remaining - record: slo:error_budget_remaining:30d expr: | 1 - ( (time() - 1640995200) / (30 * 24 * 60 * 60) * (1 - 0.9999) - (1 - sli:availability:ratio_rate30d) ) / (1 - 0.9999) --- # SLA Alerting Rules apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: sla-alerting-rules namespace: monitoring labels: prometheus: kube-prometheus role: alert-rules spec: groups: - name: sla-breach.rules rules: # Critical SLA Breach - alert: SLACriticalBreach expr: sli:availability:ratio_rate5m < 0.9995 for: 2m labels: severity: critical service: saas-idp sla_level: "99.99%" team: platform pagerduty_severity: "critical" annotations: summary: "CRITICAL: SLA breach detected" description: | Current availability: {{ $value | humanizePercentage }} SLA Target: 99.99% This is a critical SLA breach requiring immediate attention. runbook_url: "https://runbooks.company.com/sla/critical-breach" dashboard_url: "https://grafana.company.com/d/sla-dashboard" # High Burn Rate Alert - alert: ErrorBudgetHighBurnRate expr: | slo:error_budget_burn_rate:5m > 14.4 and slo:error_budget_burn_rate:1h > 14.4 for: 2m labels: severity: critical service: saas-idp team: platform pagerduty_severity: "high" annotations: summary: "High error budget burn rate" description: | Error budget burn rate: {{ $value }}x normal rate At this rate, the monthly error budget will be exhausted in {{ div 43200 $value | humanizeDuration }}. runbook_url: "https://runbooks.company.com/sla/burn-rate" # Medium Burn Rate Alert - alert: ErrorBudgetMediumBurnRate expr: | slo:error_budget_burn_rate:5m > 6 and slo:error_budget_burn_rate:1h > 6 for: 15m labels: severity: warning service: saas-idp team: platform annotations: summary: "Medium error budget burn rate" description: | Error budget burn rate: {{ $value }}x normal rate At this rate, the monthly error budget will be exhausted in {{ div 43200 $value | humanizeDuration }}. # Latency SLA Alert - alert: LatencySLABreach expr: sli:latency:p95_5m > 0.2 or sli:latency:p99_5m > 0.5 for: 5m labels: severity: warning service: saas-idp team: platform annotations: summary: "Latency SLA breach" description: | P95 latency: {{ with query "sli:latency:p95_5m" }}{{ . | first | value | humanizeDuration }}{{ end }} P99 latency: {{ with query "sli:latency:p99_5m" }}{{ . | first | value | humanizeDuration }}{{ end }} Targets: P95 < 200ms, P99 < 500ms # Error Rate SLA Alert - alert: ErrorRateSLABreach expr: sli:error_rate:ratio_rate5m > 0.001 for: 5m labels: severity: warning service: saas-idp team: platform annotations: summary: "Error rate SLA breach" description: | Current error rate: {{ $value | humanizePercentage }} Target: < 0.1% # Component-Specific Alerts - alert: DatabaseSLABreach expr: up{job="postgresql"} < 0.99995 for: 1m labels: severity: critical component: database team: platform annotations: summary: "Database availability SLA breach" description: "Database availability: {{ $value | humanizePercentage }} (Target: 99.995%)" - alert: AuthenticationSLABreach expr: rate(auth_requests_total{status=~"5.."}[5m]) / rate(auth_requests_total[5m]) > 0.00005 for: 2m labels: severity: critical component: authentication team: platform annotations: summary: "Authentication SLA breach" description: "Auth error rate: {{ $value | humanizePercentage }} (Target: < 0.005%)" --- # Multi-location Synthetic Monitoring apiVersion: monitoring.coreos.com/v1 kind: Probe metadata: name: saas-idp-global-probes namespace: monitoring spec: targets: staticConfig: static: - "https://app.saas-idp.company.com" - "https://app.saas-idp.company.com/health" - "https://app.saas-idp.company.com/api/health" - "https://app.saas-idp.company.com/login" prober: url: blackbox-exporter.monitoring.svc.cluster.local:9115 module: http_2xx interval: 30s scrapeTimeout: 10s --- # Blackbox Exporter Configuration apiVersion: v1 kind: ConfigMap metadata: name: blackbox-exporter-config namespace: monitoring data: blackbox.yml: | modules: http_2xx: prober: http timeout: 10s http: valid_http_versions: ["HTTP/1.1", "HTTP/2.0"] valid_status_codes: [200] method: GET headers: Host: app.saas-idp.company.com User-Agent: "SLA-Monitor/1.0" fail_if_ssl: false fail_if_not_ssl: true tls_config: insecure_skip_verify: false preferred_ip_protocol: "ip4" ip_protocol_fallback: false http_post_2xx: prober: http timeout: 10s http: method: POST headers: Content-Type: application/json body: '{"test": "sla-monitor"}' valid_status_codes: [200, 201] tcp_connect: prober: tcp timeout: 10s dns: prober: dns timeout: 10s dns: query_name: "app.saas-idp.company.com" query_type: "A" --- # Blackbox Exporter Deployment apiVersion: apps/v1 kind: Deployment metadata: name: blackbox-exporter namespace: monitoring labels: app: blackbox-exporter spec: replicas: 2 selector: matchLabels: app: blackbox-exporter template: metadata: labels: app: blackbox-exporter spec: containers: - name: blackbox-exporter image: prom/blackbox-exporter:latest args: - "--config.file=/etc/blackbox_exporter/config.yml" - "--web.listen-address=:9115" ports: - containerPort: 9115 name: http volumeMounts: - name: config mountPath: /etc/blackbox_exporter resources: requests: memory: "128Mi" cpu: "100m" limits: memory: "256Mi" cpu: "200m" livenessProbe: httpGet: path: /health port: 9115 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /health port: 9115 initialDelaySeconds: 5 periodSeconds: 5 volumes: - name: config configMap: name: blackbox-exporter-config --- # SLA Dashboard ConfigMap for Grafana apiVersion: v1 kind: ConfigMap metadata: name: sla-dashboard namespace: monitoring labels: grafana_dashboard: "1" data: sla-dashboard.json: | { "dashboard": { "id": null, "title": "SaaS IDP SLA Dashboard", "tags": ["saas-idp", "sla", "production"], "style": "dark", "timezone": "UTC", "refresh": "1m", "time": { "from": "now-7d", "to": "now" }, "panels": [ { "id": 1, "title": "Availability SLA Status", "type": "stat", "targets": [ { "expr": "sli:availability:ratio_rate30d * 100", "legendFormat": "30-day Availability" } ], "fieldConfig": { "defaults": { "unit": "percent", "thresholds": { "steps": [ {"color": "red", "value": 0}, {"color": "yellow", "value": 99.95}, {"color": "green", "value": 99.99} ] } } }, "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0} }, { "id": 2, "title": "Error Budget Remaining", "type": "gauge", "targets": [ { "expr": "slo:error_budget_remaining:30d * 100", "legendFormat": "Error Budget %" } ], "fieldConfig": { "defaults": { "unit": "percent", "min": 0, "max": 100, "thresholds": { "steps": [ {"color": "red", "value": 0}, {"color": "yellow", "value": 25}, {"color": "green", "value": 50} ] } } }, "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0} }, { "id": 3, "title": "Response Time SLA", "type": "timeseries", "targets": [ { "expr": "sli:latency:p95_5m * 1000", "legendFormat": "P95 Latency (ms)" }, { "expr": "sli:latency:p99_5m * 1000", "legendFormat": "P99 Latency (ms)" } ], "fieldConfig": { "defaults": { "unit": "ms", "custom": { "drawStyle": "line", "lineInterpolation": "smooth" } }, "overrides": [ { "matcher": {"id": "byName", "options": "P95 Latency (ms)"}, "properties": [ {"id": "color", "value": {"mode": "fixed", "fixedColor": "blue"}} ] } ] }, "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8} }, { "id": 4, "title": "Error Rate SLA", "type": "timeseries", "targets": [ { "expr": "sli:error_rate:ratio_rate5m * 100", "legendFormat": "Error Rate %" } ], "fieldConfig": { "defaults": { "unit": "percent", "custom": { "drawStyle": "line", "fillOpacity": 10 } } }, "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16} }, { "id": 5, "title": "Throughput", "type": "timeseries", "targets": [ { "expr": "sli:throughput:requests_per_second", "legendFormat": "Requests/sec" } ], "fieldConfig": { "defaults": { "unit": "reqps", "custom": { "drawStyle": "line", "fillOpacity": 10 } } }, "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16} } ] } } --- # SLA Incident Management Webhook apiVersion: apps/v1 kind: Deployment metadata: name: sla-incident-manager namespace: monitoring labels: app: sla-incident-manager spec: replicas: 2 selector: matchLabels: app: sla-incident-manager template: metadata: labels: app: sla-incident-manager spec: containers: - name: incident-manager image: sla-incident-manager:latest ports: - containerPort: 8080 name: http env: - name: SLACK_WEBHOOK_URL valueFrom: secretKeyRef: name: notification-secrets key: slack-webhook-url - name: PAGERDUTY_INTEGRATION_KEY valueFrom: secretKeyRef: name: pagerduty-secrets key: integration-key - name: PROMETHEUS_URL value: "http://prometheus.monitoring.svc.cluster.local:9090" - name: GRAFANA_URL value: "https://grafana.company.com" - name: SLA_TARGET value: "99.99" resources: requests: memory: "256Mi" cpu: "100m" limits: memory: "512Mi" cpu: "200m" livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 --- # Service for SLA Incident Manager apiVersion: v1 kind: Service metadata: name: sla-incident-manager namespace: monitoring labels: app: sla-incident-manager spec: selector: app: sla-incident-manager ports: - port: 80 targetPort: 8080 name: http --- # Alertmanager Configuration for SLA apiVersion: v1 kind: Secret metadata: name: alertmanager-sla-config namespace: monitoring type: Opaque stringData: alertmanager.yml: | global: slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK' pagerduty_url: 'https://events.pagerduty.com/v2/enqueue' resolve_timeout: 5m templates: - '/etc/alertmanager/templates/*.tmpl' route: group_by: ['alertname', 'service'] group_wait: 30s group_interval: 5m repeat_interval: 12h receiver: 'web.hook' routes: - match: severity: critical service: saas-idp receiver: 'sla-critical' group_wait: 10s group_interval: 2m repeat_interval: 5m - match: severity: warning service: saas-idp receiver: 'sla-warning' group_interval: 10m repeat_interval: 1h receivers: - name: 'web.hook' webhook_configs: - url: 'http://sla-incident-manager.monitoring.svc.cluster.local/webhook' - name: 'sla-critical' slack_configs: - channel: '#sla-alerts' title: ' CRITICAL SLA BREACH' text: | {{ range .Alerts }} *Alert:* {{ .Annotations.summary }} *Description:* {{ .Annotations.description }} *Runbook:* {{ .Annotations.runbook_url }} *Dashboard:* {{ .Annotations.dashboard_url }} {{ end }} send_resolved: true pagerduty_configs: - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY' description: "{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}" severity: 'critical' - name: 'sla-warning' slack_configs: - channel: '#platform-alerts' title: ' SLA Warning' text: | {{ range .Alerts }} *Alert:* {{ .Annotations.summary }} *Description:* {{ .Annotations.description }} {{ end }} send_resolved: true inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'service']