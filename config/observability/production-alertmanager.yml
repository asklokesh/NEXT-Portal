# Production AlertManager Configuration for SaaS IDP # Enhanced notification system with PagerDuty, Slack, Email, and SMS global: # SMTP Configuration smtp_smarthost: 'smtp.sendgrid.net:587' smtp_from: 'alerts@saas-idp.com' smtp_auth_username: 'apikey' smtp_auth_password: 'YOUR_SENDGRID_API_KEY' smtp_require_tls: true # Slack Configuration slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK' # PagerDuty Configuration (will be overridden per receiver) pagerduty_url: 'https://events.pagerduty.com/v2/enqueue' # Global resolve timeout resolve_timeout: 5m # HTTP client configuration http_config: tls_config: insecure_skip_verify: false follow_redirects: true # Templates for notifications templates: - '/etc/alertmanager/templates/email.tmpl' - '/etc/alertmanager/templates/slack.tmpl' - '/etc/alertmanager/templates/pagerduty.tmpl' # Main routing tree route: group_by: ['alertname', 'cluster', 'service', 'priority'] group_wait: 10s group_interval: 10s repeat_interval: 12h receiver: 'default-webhook' # Specific routing rules based on priority and service routes: # P0 CRITICAL - IMMEDIATE MULTI-CHANNEL NOTIFICATION - match: priority: P0 receiver: 'p0-critical-all-channels' group_wait: 0s group_interval: 30s repeat_interval: 5m routes: # Security P0 incidents get additional security team notification - match: service: security receiver: 'p0-security-critical' continue: true # Database P0 incidents get DBA team notification - match: service: database receiver: 'p0-database-critical' continue: true # P1 HIGH PRIORITY - ESCALATED NOTIFICATION - match: priority: P1 receiver: 'p1-high-priority' group_wait: 30s group_interval: 5m repeat_interval: 30m routes: # Business impact P1 alerts - match_re: impact: '(revenue|customer_facing|sla_breach)' receiver: 'p1-business-impact' continue: true # P2 MEDIUM PRIORITY - STANDARD NOTIFICATION - match: priority: P2 receiver: 'p2-medium-priority' group_wait: 5m group_interval: 30m repeat_interval: 2h # SECURITY ALERTS - DEDICATED SECURITY TEAM - match: service: authentication receiver: 'security-team' group_wait: 30s group_interval: 2m repeat_interval: 15m - match: attack_type: brute_force receiver: 'security-team-urgent' group_wait: 0s group_interval: 1m repeat_interval: 5m # BUSINESS METRICS - PRODUCT/BUSINESS TEAM - match: service: business receiver: 'business-team' group_wait: 10m group_interval: 1h repeat_interval: 12h # PLUGIN SPECIFIC ALERTS - match: service: plugins receiver: 'plugin-team' group_wait: 2m group_interval: 10m repeat_interval: 1h # SLO BREACH ALERTS - match_re: alertname: 'SLO_.*' receiver: 'slo-team' group_wait: 1m group_interval: 10m repeat_interval: 6h # Inhibition rules to prevent alert spam inhibit_rules: # Critical alerts silence warnings for the same service - source_match: severity: critical target_match: severity: warning equal: ['alertname', 'cluster', 'service', 'instance'] # P0 alerts silence P1/P2 for the same component - source_match: priority: P0 target_match_re: priority: 'P[12]' equal: ['service', 'component'] # System down alerts silence all other alerts for the cluster - source_match: alertname: 'P0_SystemDown' target_match_re: alertname: '.*' equal: ['cluster'] # Authentication down silences auth-related alerts - source_match: alertname: 'P0_AuthenticationSystemDown' target_match_re: service: 'authentication' equal: ['cluster'] # Database down silences database performance alerts - source_match: alertname: 'P0_DatabaseDown' target_match_re: service: 'database' equal: ['cluster'] # Time-based inhibition (silence non-critical alerts during maintenance windows) - source_match: alertname: 'MaintenanceWindow' target_match_re: priority: 'P[23]' equal: ['cluster'] # Notification receivers receivers: # DEFAULT WEBHOOK (for logging/metrics) - name: 'default-webhook' webhook_configs: - url: 'http://alertmanager-webhook:9093/webhook' send_resolved: true http_config: basic_auth: username: 'alertmanager' password: 'webhook_secret' # P0 CRITICAL - ALL CHANNELS - name: 'p0-critical-all-channels' # PagerDuty - High Urgency pagerduty_configs: - routing_key: 'PAGERDUTY_P0_INTEGRATION_KEY' description: '{{ template "pagerduty.default.description" . }}' severity: 'critical' class: 'P0' component: '{{ .GroupLabels.service }}' group: '{{ .GroupLabels.component }}' client: 'SaaS IDP Monitoring' client_url: 'https://grafana.saas-idp.com' details: summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}' description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}' runbook_url: '{{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}' affected_users: '{{ range .Alerts }}{{ .Annotations.estimated_users_affected }}{{ end }}' revenue_impact: '{{ range .Alerts }}{{ .Annotations.revenue_impact_per_minute }}{{ end }}' links: - href: 'https://grafana.saas-idp.com/d/executive-overview' text: 'Executive Dashboard' - href: 'https://status.saas-idp.com' text: 'Status Page' # Email - Multiple Recipients email_configs: - to: 'oncall-primary@saas-idp.com' subject: ' P0 CRITICAL ALERT: {{ .GroupLabels.alertname }}' body: '{{ template "email.p0.body" . }}' html: '{{ template "email.p0.html" . }}' headers: X-Priority: '1' X-MSMail-Priority: 'High' Importance: 'high' - to: 'sre-team@saas-idp.com' subject: ' P0 CRITICAL: {{ .GroupLabels.alertname }} - Immediate Action Required' body: '{{ template "email.p0.body" . }}' - to: 'executives@saas-idp.com' subject: ' CRITICAL INCIDENT: SaaS IDP Platform Issue' body: '{{ template "email.executive.body" . }}' html: '{{ template "email.executive.html" . }}' # Slack - Multiple Channels slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#alerts-critical' username: 'AlertManager' icon_emoji: ':rotating_light:' title: ' P0 CRITICAL INCIDENT' text: '{{ template "slack.p0.text" . }}' color: 'danger' actions: - type: 'button' text: 'View Grafana Dashboard' url: 'https://grafana.saas-idp.com/d/executive-overview' - type: 'button' text: 'Open War Room' url: 'https://meet.google.com/war-room-p0' - type: 'button' text: 'Update Status Page' url: 'https://status.saas-idp.com/admin' fields: - title: 'Priority' value: 'P0 - CRITICAL' short: true - title: 'Impact' value: '{{ range .Alerts }}{{ .Labels.impact }}{{ end }}' short: true - title: 'Estimated Users Affected' value: '{{ range .Alerts }}{{ .Annotations.estimated_users_affected }}{{ end }}' short: true - title: 'Revenue Impact/Min' value: '{{ range .Alerts }}{{ .Annotations.revenue_impact_per_minute }}{{ end }}' short: true # Executive Slack Channel - api_url: '{{ .SlackAPIURL }}' channel: '#executive-alerts' username: 'CriticalAlert' icon_emoji: ':fire:' title: ' PLATFORM CRITICAL INCIDENT' text: '{{ template "slack.executive.text" . }}' color: 'danger' # SMS via Twilio (webhook) webhook_configs: - url: 'https://api.twilio.com/2010-04-01/Accounts/TWILIO_ACCOUNT_SID/Messages.json' send_resolved: false http_config: basic_auth: username: 'TWILIO_ACCOUNT_SID' password: 'TWILIO_AUTH_TOKEN' max_alerts: 1 title: 'P0 CRITICAL: SaaS IDP' text: '{{ range .Alerts }}CRITICAL: {{ .Annotations.summary }}{{ end }}' # P0 SECURITY CRITICAL - name: 'p0-security-critical' # Security Team PagerDuty pagerduty_configs: - routing_key: 'PAGERDUTY_SECURITY_INTEGRATION_KEY' description: 'SECURITY BREACH: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}' severity: 'critical' class: 'Security' component: 'Security' # Security Team Email email_configs: - to: 'security-team@saas-idp.com' subject: ' SECURITY BREACH ALERT: {{ .GroupLabels.alertname }}' body: | CRITICAL SECURITY INCIDENT DETECTED {{ range .Alerts }} Alert: {{ .Annotations.summary }} Description: {{ .Annotations.description }} Time: {{ .StartsAt }} {{ if .Labels.attack_type }}Attack Type: {{ .Labels.attack_type }}{{ end }} {{ if .Labels.ip_address }}Source IP: {{ .Labels.ip_address }}{{ end }} {{ if .Labels.user_id }}User ID: {{ .Labels.user_id }}{{ end }} Runbook: {{ .Annotations.runbook_url }} {{ end }} IMMEDIATE ACTION REQUIRED - RESPOND WITHIN 2 MINUTES # Security Slack Channel slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#security-incidents' username: 'SecurityAlert' icon_emoji: ':lock:' title: ' CRITICAL SECURITY INCIDENT' text: | {{ range .Alerts }} *SECURITY BREACH DETECTED* *Alert:* {{ .Annotations.summary }} *Description:* {{ .Annotations.description }} {{ if .Labels.attack_type }}*Attack Type:* {{ .Labels.attack_type }}{{ end }} {{ if .Labels.ip_address }}*Source IP:* {{ .Labels.ip_address }}{{ end }} *Time:* {{ .StartsAt }} *Runbook:* <{{ .Annotations.runbook_url }}|Security Runbook> {{ end }} @channel IMMEDIATE RESPONSE REQUIRED color: 'danger' # P0 DATABASE CRITICAL - name: 'p0-database-critical' # DBA Team email_configs: - to: 'dba-team@saas-idp.com' subject: ' DATABASE CRITICAL: {{ .GroupLabels.alertname }}' body: | CRITICAL DATABASE ISSUE {{ range .Alerts }} Database: {{ .Labels.database }} Alert: {{ .Annotations.summary }} Description: {{ .Annotations.description }} Time: {{ .StartsAt }} Runbook: {{ .Annotations.runbook_url }} {{ end }} slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#database-alerts' username: 'DatabaseAlert' icon_emoji: ':warning:' title: ' DATABASE CRITICAL INCIDENT' # P1 HIGH PRIORITY - name: 'p1-high-priority' # PagerDuty - Medium Urgency pagerduty_configs: - routing_key: 'PAGERDUTY_P1_INTEGRATION_KEY' description: '{{ template "pagerduty.default.description" . }}' severity: 'error' class: 'P1' component: '{{ .GroupLabels.service }}' # SRE Team Email email_configs: - to: 'sre-team@saas-idp.com' subject: ' P1 HIGH PRIORITY: {{ .GroupLabels.alertname }}' body: '{{ template "email.p1.body" . }}' # Slack Alert Channel slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#alerts-high' username: 'AlertManager' icon_emoji: ':warning:' title: ' P1 High Priority Alert' text: '{{ template "slack.p1.text" . }}' color: 'warning' # P1 BUSINESS IMPACT - name: 'p1-business-impact' # Business stakeholders email_configs: - to: 'business-stakeholders@saas-idp.com' subject: ' BUSINESS IMPACT ALERT: {{ .GroupLabels.alertname }}' body: | HIGH PRIORITY BUSINESS IMPACT DETECTED {{ range .Alerts }} Impact: {{ .Labels.impact }} Alert: {{ .Annotations.summary }} Description: {{ .Annotations.description }} {{ if .Annotations.estimated_users_affected }}Users Affected: {{ .Annotations.estimated_users_affected }}{{ end }} {{ if .Annotations.revenue_impact_per_minute }}Revenue Impact/Min: {{ .Annotations.revenue_impact_per_minute }}{{ end }} Time: {{ .StartsAt }} {{ end }} slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#business-alerts' username: 'BusinessAlert' icon_emoji: ':moneybag:' title: ' Business Impact Alert' # P2 MEDIUM PRIORITY - name: 'p2-medium-priority' # Email only for P2 email_configs: - to: 'sre-team@saas-idp.com' subject: 'ℹ P2 Medium Priority: {{ .GroupLabels.alertname }}' body: '{{ template "email.p2.body" . }}' # Low priority Slack channel slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#alerts-medium' username: 'AlertManager' icon_emoji: ':information_source:' title: 'ℹ P2 Medium Priority Alert' color: 'good' # SECURITY TEAM - name: 'security-team' email_configs: - to: 'security-team@saas-idp.com' subject: ' Security Alert: {{ .GroupLabels.alertname }}' body: '{{ template "email.security.body" . }}' slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#security-alerts' username: 'SecurityMonitor' icon_emoji: ':shield:' # SECURITY TEAM URGENT - name: 'security-team-urgent' pagerduty_configs: - routing_key: 'PAGERDUTY_SECURITY_INTEGRATION_KEY' severity: 'error' email_configs: - to: 'security-team@saas-idp.com' subject: ' URGENT Security Alert: {{ .GroupLabels.alertname }}' headers: X-Priority: '1' slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#security-incidents' text: '@here URGENT SECURITY ALERT' # BUSINESS TEAM - name: 'business-team' email_configs: - to: 'product-team@saas-idp.com,business-team@saas-idp.com' subject: ' Business Metrics Alert: {{ .GroupLabels.alertname }}' body: '{{ template "email.business.body" . }}' slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#business-metrics' username: 'BusinessMonitor' icon_emoji: ':chart_with_upwards_trend:' # PLUGIN TEAM - name: 'plugin-team' email_configs: - to: 'plugin-team@saas-idp.com' subject: ' Plugin Alert: {{ .GroupLabels.alertname }}' slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#plugin-alerts' username: 'PluginMonitor' icon_emoji: ':electric_plug:' # SLO TEAM - name: 'slo-team' email_configs: - to: 'sre-team@saas-idp.com,engineering-managers@saas-idp.com' subject: ' SLO Alert: {{ .GroupLabels.alertname }}' body: | SLO BREACH OR ERROR BUDGET ALERT {{ range .Alerts }} Alert: {{ .Annotations.summary }} Description: {{ .Annotations.description }} {{ if .Labels.burn_rate }}Burn Rate: {{ .Labels.burn_rate }}{{ end }} {{ if .Annotations.sla_breach }}SLA Breach: {{ .Annotations.sla_breach }}{{ end }} Time: {{ .StartsAt }} Runbook: {{ .Annotations.runbook_url }} {{ end }} slack_configs: - api_url: '{{ .SlackAPIURL }}' channel: '#slo-monitoring' username: 'SLOMonitor' icon_emoji: ':dart:' title: ' SLO Alert'