# Quality Gates & Metrics Framework ## NEXT Internal Developer Portal ### Executive Summary This document establishes comprehensive quality gates, metrics, and KPIs to ensure consistent quality standards across the NEXT IDP platform development lifecycle. --- ## Quality Gate Architecture ### Gate Hierarchy ``` Code Commit → Pull Request → Main Branch → Staging → Production ↓ ↓ ↓ ↓ ↓ Gate 1 Gate 2 Gate 3 Gate 4 Gate 5 ``` --- ## Quality Gates Definition ### Gate 1: Pre-Commit (Local Development) **Purpose:** Catch issues before code enters version control | Check | Threshold | Tool | Action on Failure | |-------|-----------|------|-------------------| | Linting | 0 errors | ESLint | Block commit | | Type Check | 0 errors | TypeScript | Block commit | | Formatting | 100% compliance | Prettier | Auto-fix | | Unit Tests (affected) | 100% pass | Jest/Vitest | Block commit | | Commit Message | Conventional format | Commitlint | Block commit | **Implementation:** ```javascript // .husky/pre-commit #!/bin/sh npm run lint:staged npm run type-check npm run test:affected ``` ### Gate 2: Pull Request **Purpose:** Ensure code quality before merging | Check | Threshold | Tool | Action on Failure | |-------|-----------|------|-------------------| | All Unit Tests | 100% pass | Jest | Block merge | | Code Coverage | >70% overall, >50% new | Codecov | Block merge | | Integration Tests | 100% pass | Jest | Block merge | | Security Scan | No high/critical | Snyk | Block merge | | Code Review | 2 approvals | GitHub | Block merge | | Build Success | 0 errors | Next.js | Block merge | | Bundle Size | <10% increase | Bundlesize | Warning | **GitHub Actions Configuration:** ```yaml name: PR Quality Gates on: [pull_request] jobs: quality-checks: runs-on: ubuntu-latest steps: - name: Check Coverage run: | COVERAGE=$(npm run test:coverage -- --json | jq '.total.lines.pct') if (( $(echo "$COVERAGE < 70" | bc -l) )); then echo "Coverage below 70%" exit 1 fi - name: Check Bundle Size uses: andresz1/size-limit-action@v1 with: github_token: ${{ secrets.GITHUB_TOKEN }} build_script: build ``` ### Gate 3: Main Branch (Post-Merge) **Purpose:** Continuous integration validation | Check | Threshold | Tool | Action on Failure | |-------|-----------|------|-------------------| | Full Test Suite | 100% pass | All | Alert team | | E2E Tests (smoke) | 100% pass | Playwright | Block deployment | | Performance Test | No regression >10% | Lighthouse | Alert team | | Accessibility | WCAG 2.1 AA | axe-core | Warning | | Documentation | 100% complete | TypeDoc | Warning | ### Gate 4: Staging Deployment **Purpose:** Pre-production validation | Check | Threshold | Tool | Action on Failure | |-------|-----------|------|-------------------| | E2E Full Suite | >95% pass | Playwright | Block production | | Load Test | <500ms p95 | k6 | Block production | | Security Scan | No vulnerabilities | OWASP ZAP | Block production | | API Contract | 100% compliance | Pact | Block production | | Database Migration | Success | Prisma | Block production | | Smoke Tests | 100% pass | Custom | Block production | ### Gate 5: Production Deployment **Purpose:** Final validation and monitoring | Check | Threshold | Tool | Action on Failure | |-------|-----------|------|-------------------| | Health Check | All services up | Custom | Rollback | | Smoke Tests | 100% pass | Playwright | Rollback | | Error Rate | <1% increase | Sentry | Alert | | Performance | <10% degradation | DataDog | Alert | | Availability | >99.9% | Uptime | Incident | --- ## Key Performance Indicators (KPIs) ### Code Quality Metrics #### 1. Test Coverage ```javascript { "targets": { "statements": 70, "branches": 70, "functions": 70, "lines": 70 }, "critical_paths": { "auth": 90, "payments": 90, "data": 85 } } ``` **Measurement:** - Tool: Jest/Vitest with coverage reporters - Frequency: Every commit - Trending: Weekly reports #### 2. Code Complexity | Metric | Threshold | Tool | |--------|-----------|------| | Cyclomatic Complexity | <10 per function | ESLint | | Cognitive Complexity | <15 per function | SonarQube | | Lines per Function | <50 | ESLint | | Files per Module | <20 | Custom | #### 3. Technical Debt ``` Debt Ratio = (Remediation Time / Development Time) × 100 Target: < 5% Acceptable: 5-10% Critical: > 10% ``` ### Testing Metrics #### 1. Test Execution Metrics | Metric | Target | Current | Trend | |--------|--------|---------|-------| | Unit Test Duration | <5 min | - | - | | Integration Test Duration | <10 min | - | - | | E2E Test Duration | <30 min | - | - | | Total Pipeline Time | <45 min | - | - | #### 2. Test Reliability ```javascript const testReliability = { flakiness: { target: '<5%', formula: 'Flaky Tests / Total Tests × 100', action: 'Quarantine flaky tests' }, consistency: { target: '>95%', formula: 'Consistent Passes / Total Runs × 100', action: 'Fix or remove inconsistent tests' } }; ``` #### 3. Test Effectiveness | Metric | Formula | Target | |--------|---------|--------| | Defect Detection Rate | Bugs Found in Testing / Total Bugs × 100 | >90% | | Escaped Defects | Production Bugs / Total Bugs × 100 | <10% | | Test Case Effectiveness | Bugs Found / Test Cases × 100 | >5% | | Requirements Coverage | Tested Requirements / Total Requirements × 100 | 100% | ### Development Velocity Metrics #### 1. Lead Time Metrics ```mermaid graph LR A[Commit] --> B[Build] B --> C[Test] C --> D[Deploy] A -.->|Lead Time for Changes| D ``` | Metric | Target | Measurement | |--------|--------|-------------| | Lead Time for Changes | <1 day | Commit to production | | Deployment Frequency | Daily | Deployments per day | | Mean Time to Recovery | <1 hour | Incident to resolution | | Change Failure Rate | <10% | Failed deployments / total | #### 2. Sprint Metrics ```javascript const sprintMetrics = { velocity: { formula: 'Story Points Completed / Sprint', target: 'Consistent or increasing', variance: '<20%' }, defectRate: { formula: 'Defects / Story Points', target: '<0.5', trend: 'Decreasing' }, automationRate: { formula: 'Automated Tests / Total Tests', target: '>80%', growth: '+5% per sprint' } }; ``` ### Production Quality Metrics #### 1. Availability & Reliability | Metric | Target | Formula | Monitoring | |--------|--------|---------|------------| | Uptime | 99.9% | (Total Time - Downtime) / Total Time | DataDog | | MTBF | >720 hours | Total Time / Number of Failures | Custom | | MTTR | <1 hour | Total Repair Time / Number of Repairs | PagerDuty | | Error Rate | <1% | Errors / Total Requests | Sentry | #### 2. Performance Metrics ```typescript interface PerformanceTargets { web: { FCP: 1500, // First Contentful Paint (ms) LCP: 2500, // Largest Contentful Paint (ms) FID: 100, // First Input Delay (ms) CLS: 0.1, // Cumulative Layout Shift TTI: 3500 // Time to Interactive (ms) }, api: { p50: 100, // 50th percentile (ms) p95: 200, // 95th percentile (ms) p99: 500 // 99th percentile (ms) }, database: { queryTime: 50, // Average query time (ms) connectionPool: 80 // Pool utilization (%) } } ``` #### 3. User Experience Metrics | Metric | Target | Source | |--------|--------|--------| | Apdex Score | >0.85 | New Relic | | User Satisfaction | >4.0/5 | Surveys | | Task Success Rate | >90% | Analytics | | Time on Task | <2 min | Analytics | | Bounce Rate | <30% | Google Analytics | --- ## Metric Dashboards ### Engineering Dashboard ``` ┌─────────────────────────────────────────┐ │ Engineering Metrics │ ├─────────────┬──────────┬────────────────┤ │ Coverage │ Pipeline │ Sprint │ │ ■■■■■■■□□□ │ Pass │ Velocity: 45 │ │ 70% │ 23 min │ Bugs: 3 │ ├─────────────┴──────────┴────────────────┤ │ Test Results │ │ Unit: 1,245 12 │ │ Integration: 234 2 │ │ E2E: 45 1 │ ├──────────────────────────────────────────┤ │ Quality Trends │ │ [Graph showing coverage over time] │ └──────────────────────────────────────────┘ ``` ### Production Dashboard ``` ┌─────────────────────────────────────────┐ │ Production Metrics │ ├─────────────┬──────────┬────────────────┤ │ Uptime │ Errors │ Performance │ │ 99.95% │ 0.3% │ P95: 187ms │ ├─────────────┴──────────┴────────────────┤ │ Real-time Status │ │ API: DB: Cache: CDN: │ ├──────────────────────────────────────────┤ │ Recent Incidents │ │ • [None in last 7 days] │ └──────────────────────────────────────────┘ ``` --- ## Alerting & Thresholds ### Alert Configuration ```yaml alerts: - name: High Error Rate condition: error_rate > 5% duration: 5 minutes severity: critical notify: ['pagerduty', 'slack'] - name: Performance Degradation condition: p95_latency > 500ms duration: 10 minutes severity: warning notify: ['slack'] - name: Test Coverage Drop condition: coverage < 65% severity: warning notify: ['email', 'slack'] - name: Failed Deployment condition: deployment_status == 'failed' severity: critical notify: ['pagerduty', 'slack', 'email'] ``` ### Escalation Matrix | Severity | Response Time | Escalation Path | Communication | |----------|---------------|-----------------|---------------| | Critical | Immediate | Dev → Lead → Manager → VP | PagerDuty + Slack | | High | 15 minutes | Dev → Lead → Manager | Slack + Email | | Medium | 1 hour | Dev → Lead | Slack | | Low | 4 hours | Dev | Email | --- ## Continuous Improvement Process ### Metric Review Cycle ``` Daily: Error rates, availability, performance Weekly: Test coverage, sprint velocity, defect rates Monthly: Technical debt, user satisfaction, trends Quarterly: KPI targets, tool effectiveness, process improvements ``` ### Improvement Actions 1. **Metric Below Target** - Root cause analysis - Action plan creation - Implementation tracking - Effectiveness measurement 2. **Trend Analysis** ```javascript function analyzeTrend(metric, period = 30) { const trend = calculateTrend(metric, period); if (trend.direction === 'negative' && trend.significance > 0.1) { createImprovementTicket({ metric: metric.name, current: metric.value, target: metric.target, trend: trend }); } } ``` 3. **Benchmark Comparison** | Metric | Industry Standard | Our Target | Current | |--------|------------------|------------|---------| | Test Coverage | 60-80% | 70% | - | | Deployment Frequency | Weekly | Daily | - | | MTTR | 4 hours | 1 hour | - | | Change Failure Rate | 15% | 10% | - | --- ## Quality Score Calculation ### Composite Quality Score ```javascript const qualityScore = { components: [ { name: 'Coverage', weight: 0.25, value: 0.70 }, { name: 'Test Pass Rate', weight: 0.20, value: 0.98 }, { name: 'Performance', weight: 0.20, value: 0.85 }, { name: 'Security', weight: 0.15, value: 0.95 }, { name: 'Availability', weight: 0.10, value: 0.999 }, { name: 'User Satisfaction', weight: 0.10, value: 0.80 } ], calculate() { return this.components.reduce((score, component) => { return score + (component.value * component.weight); }, 0); }, grade() { const score = this.calculate(); if (score >= 0.9) return 'A'; if (score >= 0.8) return 'B'; if (score >= 0.7) return 'C'; if (score >= 0.6) return 'D'; return 'F'; } }; // Current Score: B (0.84) ``` --- ## Reporting & Communication ### Report Templates #### Weekly Quality Report ```markdown # Weekly Quality Report - Week of [Date] ## Executive Summary - Quality Score: B (0.84) - Test Coverage: 70% (+2%) - Defects Found: 12 (-3) - Production Incidents: 0 ## Highlights - Achieved 70% coverage target - Zero production incidents - E2E test duration increased by 15% ## Action Items 1. Optimize E2E test suite 2. Fix 3 flaky tests 3. Improve dashboard performance ## Metrics Trend [Coverage Graph] [Defect Graph] [Performance Graph] ``` #### Monthly Executive Dashboard ``` ┌────────────────────────────────────────────┐ │ Quality Metrics Summary │ ├────────────────────────────────────────────┤ │ Overall Quality Score: B (0.84) ↑ │ │ Coverage: 70% ↑ │ │ Availability: 99.95% → │ │ Customer Satisfaction: 4.2/5 ↑ │ ├────────────────────────────────────────────┤ │ Risk Areas │ │ • Performance degradation in dashboard │ │ • Increasing test execution time │ │ • Technical debt in legacy modules │ └────────────────────────────────────────────┘ ``` --- ## Implementation Checklist ### Phase 1: Foundation (Week 1) - [ ] Set up quality gate configurations - [ ] Configure CI/CD pipeline checks - [ ] Install monitoring tools - [ ] Create metric collection scripts - [ ] Set up dashboards ### Phase 2: Automation (Week 2-3) - [ ] Automate metric collection - [ ] Configure alerting rules - [ ] Set up reporting automation - [ ] Integrate with communication tools - [ ] Create baseline measurements ### Phase 3: Optimization (Week 4+) - [ ] Tune thresholds based on data - [ ] Optimize slow-running checks - [ ] Implement trend analysis - [ ] Create improvement workflows - [ ] Regular review and adjustment --- ## Success Criteria ### Short-term (1 Month) - All quality gates operational - Automated metric collection - Daily dashboard updates - Alert system functional - Baseline metrics established ### Medium-term (3 Months) - Quality score > B (0.80) - Test coverage > 70% - MTTR < 2 hours - Zero critical production bugs - Fully automated reporting ### Long-term (6 Months) - Quality score > A (0.90) - Test coverage > 80% - MTTR < 1 hour - Deployment frequency: Multiple daily - Industry-leading metrics --- ## Conclusion This quality gates and metrics framework provides: 1. **Clear Standards** - Defined thresholds at each stage 2. **Automated Enforcement** - Gates prevent quality degradation 3. **Continuous Monitoring** - Real-time visibility into quality 4. **Data-Driven Decisions** - Metrics guide improvements 5. **Accountability** - Clear ownership and escalation By implementing these quality gates and tracking these metrics, the NEXT IDP platform will maintain high quality standards while enabling rapid, confident delivery of new features.